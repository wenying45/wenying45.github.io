<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wei Pan on Ying WEN</title>
    <link>https://yingwen.io/authors/wei-pan/</link>
    <description>Recent content in Wei Pan on Ying WEN</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019</copyright>
    <lastBuildDate>Tue, 01 Jan 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yingwen.io/authors/wei-pan/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning</title>
      <link>https://yingwen.io/publication/pr2k/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://yingwen.io/publication/pr2k/</guid>
      <description>In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents&amp;rsquo; conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.  Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.</description>
    </item>
    
  </channel>
</rss>