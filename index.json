[{"authors":["Ying Wen","Yaodong Yang","Rui Luo","Jun Wang","Wei Pan"],"categories":null,"content":"","date":1545350400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545350400,"objectID":"84d5ce6824894e52549bf81f636ebec0","permalink":"https://yingwen.io/publication/pr2k/","publishdate":"2018-12-21T00:00:00Z","relpermalink":"/publication/pr2k/","section":"publication","summary":"Humans are capable of attributing latent mental contents such as beliefs, or intentions to others. The social skill is critical in everyday life to reason about the potential consequences of their behaviors so as to plan ahead. It is known that humans use this reasoning ability recursively, i.e. considering what others believe about their own beliefs. In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.  Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.","tags":["MARL"],"title":"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning","type":"publication"},{"authors":["Yaodong Yang","Lantao Yu","Yiwei Bai","Ying Wen","Weinan Zhang","Jun Wang"],"categories":null,"content":"","date":1531090800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531090800,"objectID":"cdec19ef9c90bfb41ed6e4dfe6c93774","permalink":"https://yingwen.io/publication/1m-agent/","publishdate":"2018-07-09T00:00:00+01:00","relpermalink":"/publication/1m-agent/","section":"publication","summary":"We conduct an empirical study on discovering the ordered collective dynamics obtained by a population of intelligence agents, driven by million-agent reinforcement learning. Our intention is to put intelligent agents into a simulated natural context and verify if the principles developed in the real world could also be used in understanding an artificially-created intelligent population. To achieve this, we simulate a large-scale predator-prey world, where the laws of the world are designed by only the findings or logical equivalence that have been discovered in nature. We endow the agents with the intelligence based on deep reinforcement learning (DRL). In order to scale the population size up to millions agents, a large-scale DRL training platform with redesigned experience buffer is proposed. Our results show that the population dynamics of AI agents, driven only by each agent's individual self-interest, reveals an ordered pattern that is similar to the Lotka-Volterra model studied in population biology. We further discover the emergent behaviors of collective adaptations in studying how the agents' grouping behaviors will change with the environmental resources. Both of the two findings could be explained by the self-organization theory in nature.","tags":["MARL"],"title":"A Study of AI Population Dynamics with Million-agent Reinforcement Learning","type":"publication"},{"authors":["Haifeng Zhang","Jun Wang","Zhiming Zhou","Weinan Zhang","Ying Wen","Yong Yu","Wenxin Li"],"categories":null,"content":"","date":1530745200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530745200,"objectID":"1221e3bfa705950d36a45ab8a7941441","permalink":"https://yingwen.io/publication/env-rl/","publishdate":"2018-07-05T00:00:00+01:00","relpermalink":"/publication/env-rl/","section":"publication","summary":"In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment. In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. Theoretically, we find a dual Markov decision process (MDP) wrt the environment to that wrt the agent, and solving the dual MDP-policy pair yields a policy gradient solution to optimizing the parametrized environment. Furthermore, environments with non-differentiable parameters are addressed by a proposed general generative framework. Experiments on a Maze generation task show the effectiveness of generating diverse and challenging Mazes against agents with various settings.","tags":["MARL"],"title":"Learning to Design Games: Strategic Environments in Deep Reinforcement Learning","type":"publication"},{"authors":["Ying Wen"],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways: - **Create** slides using Academic's *Slides* feature and link using `url_slides` parameter in the front matter of the talk file - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/). Further talk details can easily be added to this page using *Markdown* and $\\rm \\LaTeX$ math code. -- ","date":1527202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527202800,"objectID":"f085338f540e47f373edb01ba77658c1","permalink":"https://yingwen.io/talk/yunqi2018/","publishdate":"2018-05-25T00:00:00+01:00","relpermalink":"/talk/yunqi2018/","section":"talk","summary":"Most successful researches on reinforcement learning have been in single agent domain. However, many complex reinforcement learning problems such as multiplayer games, machine bidding in competitive e-commerce and financial markets are naturally modelled...","tags":[],"title":"Communication in Multi-agent Reinforcement Learning ","type":"talk"},{"authors":["Ying Wen","Peng Peng","Yaodong Yang","Quan Yuan","Zhenkun Tang","Haitao Long","Jun Wang"],"categories":null,"content":"","date":1490742000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490742000,"objectID":"641b038b53028b6165f0fe6020af644d","permalink":"https://yingwen.io/publication/game-ai/","publishdate":"2017-03-29T00:00:00+01:00","relpermalink":"/publication/game-ai/","section":"publication","summary":"Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.","tags":["MARL"],"title":"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games","type":"publication"},{"authors":["Yanru Qu","Han Cai","Kan Ren","Weinan Zhang","Yong Yu","Ying Wen","Jun Wang"],"categories":null,"content":"","date":1481500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481500800,"objectID":"6f91d5b5c912367ded885905859ab91b","permalink":"https://yingwen.io/publication/pnn/","publishdate":"2016-12-12T00:00:00Z","relpermalink":"/publication/pnn/","section":"publication","summary":"Predicting user responses, such as clicks and conversions, is of great importance and has found its usage inmany Web applications including recommender systems, webs earch and online advertising. The data in those applications is mostly categorical and contains multiple fields, a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfieldcategories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two-large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.","tags":["IR"],"title":"Product-based Neural Networks for User Response Prediction","type":"publication"},{"authors":["Ying Wen","Weinan Zhang","Rui Luo","Jun Wang"],"categories":null,"content":"","date":1466550000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466550000,"objectID":"9454627c28bbbf454bac3972cbd7a97a","permalink":"https://yingwen.io/publication/rcnn-hw/","publishdate":"2016-06-22T00:00:00+01:00","relpermalink":"/publication/rcnn-hw/","section":"publication","summary":"Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.","tags":["IR"],"title":"Learning Text Tepresentation Using Recurrent Convolutional Neural Network with Highway Layers","type":"publication"}]