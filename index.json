[{"authors":["admin"],"categories":null,"content":"Ying Wen is a PhD Student in the Department of Computer Science, University College London. His research interests include deep learning and deep reinforcement learning techniques for real-world scenarios, such as computational advertising, recommender systems, multi-agent system. Ying earned his MRes(Master of Research) with Distinction Honor from University College London in 2016 and B.Eng. with First Class Honor from Queen Mary, University of London and Beijing University of Posts and Tel. in 2015. He was an intern at MediaGamma, Amazon and Baidu.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://yingwen.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Ying Wen is a PhD Student in the Department of Computer Science, University College London. His research interests include deep learning and deep reinforcement learning techniques for real-world scenarios, such as computational advertising, recommender systems, multi-agent system. Ying earned his MRes(Master of Research) with Distinction Honor from University College London in 2016 and B.Eng. with First Class Honor from Queen Mary, University of London and Beijing University of Posts and Tel.","tags":null,"title":"Ying Wen","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536447600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536447600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://yingwen.io/tutorial/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Ying Wen","Yaodong Yang","Rui Luo","Jun Wang"],"categories":null,"content":"","date":1548720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548720000,"objectID":"1c8a35ebc5ad15dc29e89e59e2991418","permalink":"https://yingwen.io/publication/gr2/","publishdate":"2019-01-29T00:00:00Z","relpermalink":"/publication/gr2/","section":"publication","summary":"We propose a new reasoning protocol called generalized recursive reasoning (GR2), and embed it into the multi-agent reinforcement learning (MARL) framework. The GR2 model defines reasoning categories: level-0 agent acts randomly, and level-k agent takes the best response to a mixed type of agents that are distributed over level 0 to kâˆ’1. The GR2 leaners can take into account the bounded rationality, and it does not need the assumption that the opponent agents play Nash strategy in all stage games, which many MARL algorithms require. We prove that when the level k is large, the GR2 learners will converge to at least one Nash Equilibrium (NE). In addition, if lower-level agents play the NE, high-level agents will surely follow as well. We evaluate the GR2 Soft Actor-Critic algorithms in a series of games and high-dimensional environment; results show that the GR2 methods have faster convergence speed than strong MARL baselines.","tags":["MARL"],"title":"Multi-Agent Generalized Recursive Reasoning","type":"publication"},{"authors":["Ying Wen","Yaodong Yang","Rui Luo","Jun Wang","Wei Pan"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"84d5ce6824894e52549bf81f636ebec0","permalink":"https://yingwen.io/publication/pr2k/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/pr2k/","section":"publication","summary":"In this paper, we introduce a probabilistic recursive reasoning (PR2) framework for multi-agent reinforcement learning. Our hypothesis is that it is beneficial for each agent to account for how the opponents would react to its future behaviors. Under the PR2 framework, we adopt variational Bayes methods to approximate the opponents' conditional policy, to which each agent finds the  best response and then improve their own policy. We develop  decentralized-training-decentralized-execution  algorithms, PR2-Q and PR2-Actor-Critic, that are proved to converge in the self-play scenario when there is one Nash equilibrium.  Our methods are tested on both the matrix game and the differential game, which have a non-trivial equilibrium where common gradient-based methods fail to converge. Our experiments show that it is critical to reason about how the opponents believe about what the agent believes. We expect our work to contribute a new idea of modeling the opponents to the multi-agent reinforcement learning community.","tags":["MARL"],"title":"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning","type":"publication"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536447600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536447600,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://yingwen.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Tutorial","type":"docs"},{"authors":["Yaodong Yang","Lantao Yu","Yiwei Bai","Ying Wen","Weinan Zhang","Jun Wang"],"categories":null,"content":"","date":1531090800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531090800,"objectID":"cdec19ef9c90bfb41ed6e4dfe6c93774","permalink":"https://yingwen.io/publication/1m-agent/","publishdate":"2018-07-09T00:00:00+01:00","relpermalink":"/publication/1m-agent/","section":"publication","summary":"We conduct an empirical study on discovering the ordered collective dynamics obtained by a population of intelligence agents, driven by million-agent reinforcement learning. Our intention is to put intelligent agents into a simulated natural context and verify if the principles developed in the real world could also be used in understanding an artificially-created intelligent population. To achieve this, we simulate a large-scale predator-prey world, where the laws of the world are designed by only the findings or logical equivalence that have been discovered in nature. We endow the agents with the intelligence based on deep reinforcement learning (DRL). In order to scale the population size up to millions agents, a large-scale DRL training platform with redesigned experience buffer is proposed. Our results show that the population dynamics of AI agents, driven only by each agent's individual self-interest, reveals an ordered pattern that is similar to the Lotka-Volterra model studied in population biology. We further discover the emergent behaviors of collective adaptations in studying how the agents' grouping behaviors will change with the environmental resources. Both of the two findings could be explained by the self-organization theory in nature.","tags":["MARL"],"title":"A Study of AI Population Dynamics with Million-agent Reinforcement Learning","type":"publication"},{"authors":["Haifeng Zhang","Jun Wang","Zhiming Zhou","Weinan Zhang","Ying Wen","Yong Yu","Wenxin Li"],"categories":null,"content":"","date":1530745200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530745200,"objectID":"1221e3bfa705950d36a45ab8a7941441","permalink":"https://yingwen.io/publication/env-rl/","publishdate":"2018-07-05T00:00:00+01:00","relpermalink":"/publication/env-rl/","section":"publication","summary":"In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment. In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. Theoretically, we find a dual Markov decision process (MDP) wrt the environment to that wrt the agent, and solving the dual MDP-policy pair yields a policy gradient solution to optimizing the parametrized environment. Furthermore, environments with non-differentiable parameters are addressed by a proposed general generative framework. Experiments on a Maze generation task show the effectiveness of generating diverse and challenging Mazes against agents with various settings.","tags":["MARL"],"title":"Learning to Design Games: Strategic Environments in Deep Reinforcement Learning","type":"publication"},{"authors":["Ying Wen"],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.\n  Slides can be added in a few ways: - **Create** slides using Academic's *Slides* feature and link using `url_slides` parameter in the front matter of the talk file - **Upload** an existing slide deck to `static/` and link using `url_slides` parameter in the front matter of the talk file - **Embed** your slides (e.g. Google Slides) or presentation video on this page using [shortcodes](https://sourcethemes.com/academic/docs/writing-markdown-latex/). Further talk details can easily be added to this page using *Markdown* and $\\rm \\LaTeX$ math code. -- ","date":1527202800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527202800,"objectID":"f085338f540e47f373edb01ba77658c1","permalink":"https://yingwen.io/talk/yunqi2018/","publishdate":"2018-05-25T00:00:00+01:00","relpermalink":"/talk/yunqi2018/","section":"talk","summary":"Most successful researches on reinforcement learning have been in single agent domain. However, many complex reinforcement learning problems such as multiplayer games, machine bidding in competitive e-commerce and financial markets are naturally modelled...","tags":[],"title":"Communication in Multi-agent Reinforcement Learning ","type":"talk"},{"authors":["Ying Wen","Peng Peng","Yaodong Yang","Quan Yuan","Zhenkun Tang","Haitao Long","Jun Wang"],"categories":null,"content":"","date":1490742000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490742000,"objectID":"641b038b53028b6165f0fe6020af644d","permalink":"https://yingwen.io/publication/game-ai/","publishdate":"2017-03-29T00:00:00+01:00","relpermalink":"/publication/game-ai/","section":"publication","summary":"Many artificial intelligence (AI) applications often require multiple intelligent agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI. In this paper, we take StarCraft combat game as a case study, where the task is to coordinate multiple agents as a team to defeat their enemies. To maintain a scalable yet effective communication protocol, we introduce a Multiagent Bidirectionally-Coordinated Network (BiCNet ['bIknet]) with a vectorised extension of actor-critic formulation. We show that BiCNet can handle different types of combats with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of advanced coordination strategies that have been commonly used by experienced game players. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.","tags":["MARL"],"title":"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games","type":"publication"},{"authors":["Yanru Qu","Han Cai","Kan Ren","Weinan Zhang","Yong Yu","Ying Wen","Jun Wang"],"categories":null,"content":"","date":1481500800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1481500800,"objectID":"6f91d5b5c912367ded885905859ab91b","permalink":"https://yingwen.io/publication/pnn/","publishdate":"2016-12-12T00:00:00Z","relpermalink":"/publication/pnn/","section":"publication","summary":"Predicting user responses, such as clicks and conversions, is of great importance and has found its usage inmany Web applications including recommender systems, webs earch and online advertising. The data in those applications is mostly categorical and contains multiple fields, a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfieldcategories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two-large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.","tags":["IR"],"title":"Product-based Neural Networks for User Response Prediction","type":"publication"},{"authors":["Ying Wen","Weinan Zhang","Rui Luo","Jun Wang"],"categories":null,"content":"","date":1466550000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1466550000,"objectID":"9454627c28bbbf454bac3972cbd7a97a","permalink":"https://yingwen.io/publication/rcnn-hw/","publishdate":"2016-06-22T00:00:00+01:00","relpermalink":"/publication/rcnn-hw/","section":"publication","summary":"Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.","tags":["IR"],"title":"Learning Text Representation Using Recurrent Convolutional Neural Network with Highway Layers","type":"publication"},{"authors":[],"categories":null,"content":" $$a^2 + b^2 = c^2$$\nAcademic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461106800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515801600,"objectID":"b53d50d26b084fdbeca68f75456a14a0","permalink":"https://yingwen.io/post/mapr2/","publishdate":"2016-04-20T00:00:00+01:00","relpermalink":"/post/mapr2/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"}]